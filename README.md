# Eapteka Тестовые задания

## Инженер данных
- Папка DE содержит задания на должность DE (Data engineer)
- Структура заданий:
  - [DWH](DE/1.%20DWH.md)
  - [ETL](DE/2.%20ETL.md)
  - [SQL](DE/3.%20SQL.md)
  - [CICD ](DE/4.%20cicd.md)
- Запустите docker-compose файл для работы. 
- Что бы прислать ответ вам необходимо:
  -  Сделать форк репозитория
  - Сделать PR ваших ответов в **своем** склонированном репозитории (пожалуйста не делайте PR тут что бы ответы не были видны другим)
  - Прислать нам ссылку на свой PR


- В случае вопросов или ошибок заводите Issue.

## Описание вакансии
<details> 
  <summary>Подробнее</summary>
  Наша команда разрабатывает, эксплуатирует и расширяет инфраструктуру Data Lake/Data Driven BI, позволяя Компании принимать обоснованные аналитические и технологические решения для высоконагруженных технологических процессов. 
  Компания активно развивается в экосистеме Сбера. Вот почему мы ищем инженеров, которые помогут нашей фантастической команде быстро расти и достигать новых высот в области обработки данных.

  Наши технологии и необходимый опыт работы:

  - S3, Grafana, GitLab, Kafka.
  - Greenplum, Airflow, dbt, Datahub, (будет плюсом Elasticsearch, DataLens, Metabase).
  - Python на уровне middle+ или senior
  - k8s на уровне пользователя (будет преимуществом)


  Обязанности:
  - Управлять инфраструктурой обработки данных для аналитики и обслуживания пользователей.
  - Разрабатывать процессы и интерфейсы обработки данных, полученных из исходных источников продукта и API. Обработка событий из Kafka, YandexMarket и декомпозиции их в представление данных
  - Настройка, оптимизация, обновление и контроль качества данных.
  - Разработка гибкой и удобной архитектуры DWH для бизнес-аналитиков. Обеспечение стабильности и надежности работы DWH.
  - Полный цикл получения задания, обработки и формирование пользовательской документации по существующим и новым источникам данных. Качественная интеграция в существующие модели данных.
  - Разработка пайплайнов обработки данных как в batch режиме (PySpark), так и в потоке (Snowplow).
  - Оптимизация существующих пайплайнов. 
  - Разработка Feature Store для DS


  Квалификации:
  - Опыт работы в компаниях, активно использующих современный технологический стек. 
  - Специализация не менее 3 лет обработки высоконагруженных потоков данных  
</details>

